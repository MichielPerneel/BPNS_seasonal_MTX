{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import dash_bio\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn3\n",
    "import scipy.signal as signal\n",
    "import venn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll have a look at some general patterns of expression of the assembled transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "expression = pd.read_csv('../../data/kallisto/tpm.csv', engine='pyarrow', index_col=0)\n",
    "transcripts_per_L = pd.read_csv('../../data/kallisto/transcripts_per_L.csv', engine='pyarrow', index_col=0)\n",
    "\n",
    "# Load the metadata\n",
    "meta = pd.read_csv('../../samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by exploring the expression distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate row sums\n",
    "sum = expression.sum(axis=1).sort_values(ascending=False)\n",
    "sum_transcripts_per_L = transcripts_per_L.sum(axis=1).sort_values(ascending=False)\n",
    "not_expressed = len(sum[sum == 0])\n",
    "lowly_expressed = len(sum[sum < 1])\n",
    "total = len(sum)\n",
    "print(f'In total {len(expression)} transcripts have been assembled')\n",
    "print(f'{(not_expressed / total) * 100} percent of assembled transcripts are not detected in TPM')\n",
    "print(f'{(lowly_expressed / total) * 100} percent of assembled transcripts are not expressed above 1 TPM in all samples')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation of transcripts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the percentage of assembled transcripts for which we found some annotation information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the functional annotation data\n",
    "functional_annotation = pd.read_csv('../../data/annotation/functional_eggnog/functional_annotation.emapper.annotations', engine='pyarrow', sep='\\t')\n",
    "# Fix transcript names in the first column so that they equal the transcript identifiers in the count files\n",
    "functional_annotation.iloc[:, 0] = functional_annotation.iloc[:, 0].str.split(\".\", expand=True).drop(columns=1)\n",
    "\n",
    "# Load taxonomic information\n",
    "taxonomy = pd.read_table('../../data/annotation/taxonomy_phyloDB_extended/phylodb_1.076.taxonomy_extended.txt')\n",
    "annotation = pd.read_table('../../data/annotation/taxonomy_phyloDB_extended/phylodb_1.076.annotations_extended.txt', engine='pyarrow', header=None)\n",
    "annotation.columns = ['target_id', 'code', 'strain_name', 'function']\n",
    "# load the annotation alignment data\n",
    "taxonomic_annotation_90 = pd.read_table('../../data/annotation/taxonomy_phyloDB_extended/phylodb_extended.firsthit.90plus_alnscore.m8', engine='pyarrow', header=None)\n",
    "taxonomic_annotation_60 = pd.read_table('../../data/annotation/taxonomy_phyloDB_extended/phylodb_extended.firsthit.60plus_alnscore.m8', engine='pyarrow', header=None)\n",
    "\n",
    "taxonomic_annotation_90.columns = ['query_id', 'target_id', 'p_ident', 'alnlen', 'mismatch', 'gapopen', 'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bits']\n",
    "# Fix transcript names in the first column so that they equal the transcript identifiers in the count files\n",
    "taxonomic_annotation_90.iloc[:, 0] = taxonomic_annotation_90.iloc[:, 0].str.split(\".\", expand=True).drop(columns=1)\n",
    "# Add the taxonomy information to the taxonomic_annotation_90\n",
    "# taxonomic_annotation_90 = taxonomic_annotation_90.merge(annotation, left_on='target_id', right_on='target_id', how='left')\n",
    "taxonomic_annotation_90 = taxonomic_annotation_90.merge(annotation, left_on='target_id', right_on='target_id')\n",
    "taxonomic_annotation_90 = taxonomic_annotation_90.merge(taxonomy, left_on='strain_name', right_on = 'strain_name')\n",
    "taxonomic_annotation_90 = taxonomic_annotation_90.drop(columns=['code', 'peptide_count', 'mismatch', 'gapopen', 'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bits'])\n",
    "# Expand taxonomy path in relevant columns\n",
    "taxonomic_annotation_90[['kingdom', 'superphylum', 'phylum', 'class', 'order', 'family', 'genus', 'species']] = taxonomic_annotation_90['taxonomy'].str.split(';', expand = True)\n",
    "\n",
    "\n",
    "taxonomic_annotation_60.columns = ['query_id', 'target_id', 'p_ident', 'alnlen', 'mismatch', 'gapopen', 'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bits']\n",
    "# Fix transcript names in the first column so that they equal the transcript identifiers in the count files\n",
    "taxonomic_annotation_60.iloc[:, 0] = taxonomic_annotation_60.iloc[:, 0].str.split(\".\", expand=True).drop(columns=1)\n",
    "# Add the taxonomy information to the taxonomic_annotation_60\n",
    "# taxonomic_annotation_60 = taxonomic_annotation_60.merge(annotation, left_on='target_id', right_on='target_id', how='left')\n",
    "taxonomic_annotation_60 = taxonomic_annotation_60.merge(annotation, left_on='target_id', right_on='target_id')\n",
    "taxonomic_annotation_60 = taxonomic_annotation_60.merge(taxonomy, left_on='strain_name', right_on = 'strain_name')\n",
    "taxonomic_annotation_60 = taxonomic_annotation_60.drop(columns=['code', 'peptide_count', 'mismatch', 'gapopen', 'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bits'])\n",
    "# Expand taxonomy path in relevant columns\n",
    "taxonomic_annotation_60[['kingdom', 'superphylum', 'phylum', 'class', 'order', 'family', 'genus', 'species']] = taxonomic_annotation_60['taxonomy'].str.split(';', expand = True)\n",
    "\n",
    "# Load the EukProt annotation data\n",
    "taxonomic_annotation_90_eukprot = pd.read_table('../../data/annotation/taxonomy_eukprot/eukprot_DB.firsthit.90plus_alnscore.m8', engine='pyarrow', header=None)\n",
    "taxonomic_annotation_60_eukprot = pd.read_table('../../data/annotation/taxonomy_eukprot/eukprot_DB.firsthit.60plus_alnscore.m8', engine='pyarrow', header=None)\n",
    "\n",
    "# Fix transcript names in the first column so that they equal the transcript identifiers in the count files\n",
    "taxonomic_annotation_90_eukprot.iloc[:, 0] = taxonomic_annotation_90_eukprot.iloc[:, 0].str.split(\".\", expand=True).drop(columns=1)\n",
    "taxonomic_annotation_60_eukprot.iloc[:, 0] = taxonomic_annotation_60_eukprot.iloc[:, 0].str.split(\".\", expand=True).drop(columns=1)\n",
    "\n",
    "## In the second column, split of the EukProt ID off\n",
    "eukprot_ID = taxonomic_annotation_90_eukprot.iloc[:, 1].str.split(\"_\", expand=True)[0]\n",
    "taxonomic_annotation_90_eukprot.iloc[:, 1] = eukprot_ID\n",
    "taxonomic_annotation_90_eukprot.columns = ['query_id', 'target_id', 'p_ident', 'alnlen', 'mismatch', 'gapopen', 'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bits']\n",
    "\n",
    "eukprot_ID = taxonomic_annotation_60_eukprot.iloc[:, 1].str.split(\"_\", expand=True)[0]\n",
    "taxonomic_annotation_60_eukprot.iloc[:, 1] = eukprot_ID\n",
    "taxonomic_annotation_60_eukprot.columns = ['query_id', 'target_id', 'p_ident', 'alnlen', 'mismatch', 'gapopen', 'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bits']\n",
    "\n",
    "## Add taxonomic information\n",
    "eukprot_taxonomy = pd.read_table('../../data/annotation/taxonomy_eukprot/EukProt_included_data_sets.v03.2021_11_22.txt')\n",
    "print(f'The eukprot taxonomy file contains {len(eukprot_taxonomy)} rows')\n",
    "\n",
    "# Drop the columns that are not needed\n",
    "eukprot_taxonomy.drop(columns=['Previous_Names', 'Replaces_EukProt_ID', 'Data_Source_URL', 'Data_Source_Name', 'Paper_DOI', 'Actions_Prior_to_Use',\n",
    "       'Data_Source_Type', 'Notes', 'Columns_Modified_Since_Previous_Version', 'Merged_Strains',\n",
    "       'Alternative_Strain_Names', '18S_Sequence_GenBank_ID', '18S_Sequence',\n",
    "       '18S_Sequence_Source', '18S_Sequence_Other_Strain_GenBank_ID',\n",
    "       '18S_Sequence_Other_Strain_Name', '18S_and_Taxonomy_Notes'], inplace=True)\n",
    "\n",
    "# Swap the _ to a space in the Name_to_Use column\n",
    "eukprot_taxonomy['Name_to_Use'] = eukprot_taxonomy['Name_to_Use'].str.replace('_', ' ')\n",
    "\n",
    "# Merge the annotation and taxonomy files\n",
    "taxonomic_annotation_90_eukprot = taxonomic_annotation_90_eukprot.merge(eukprot_taxonomy, left_on='target_id', right_on='EukProt_ID', how='left')\n",
    "taxonomic_annotation_60_eukprot = taxonomic_annotation_60_eukprot.merge(eukprot_taxonomy, left_on='target_id', right_on='EukProt_ID', how='left')\n",
    "\n",
    "# Drop the columns that are not needed\n",
    "taxonomic_annotation_90_eukprot.drop(columns=['target_id', 'gapopen', 'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bits'], inplace=True)\n",
    "taxonomic_annotation_60_eukprot.drop(columns=['target_id', 'gapopen', 'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bits'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_annotation_60.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_annotation_60_eukprot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the transcripts in the expression data have a certain annotation\n",
    "\n",
    "## Get the total number of predicted proteins or transcripts\n",
    "total_transcripts = len(expression)\n",
    "total_proteins = 3705883\n",
    "\n",
    "## Create a dataframe to store the results\n",
    "annotation_found = pd.DataFrame(columns=['annotated', 'unannotated', 'total_proteins'])\n",
    "\n",
    "## Define the annotations of interest\n",
    "annotation_of_interest = ['KEGG_ko', 'PFAMs', 'GOs', 'EC', 'KEGG_Pathway', 'KEGG_Module', 'max_annot_lvl', 'COG_category']\n",
    "\n",
    "## Populate the dataframe\n",
    "for annotation in annotation_of_interest:\n",
    "    annotated = len(functional_annotation[functional_annotation[annotation] != '-'][annotation].notna())\n",
    "    unannotated = total_proteins - annotated\n",
    "    annotation_found.loc[annotation] = [annotated, unannotated, total_proteins]\n",
    "\n",
    "# Do the same thing for the taxonomic annotation\n",
    "annotation_of_interest = ['taxonomy']\n",
    "\n",
    "for annotation in annotation_of_interest:\n",
    "    # 90% identity\n",
    "    annotated = len(taxonomic_annotation_90[taxonomic_annotation_90[annotation] != '-'][annotation].notna())\n",
    "    unannotated = total_proteins - annotated\n",
    "    annotation_found.loc[annotation + '_90'] = [annotated, unannotated, total_proteins]\n",
    "    # 60% identity\n",
    "    annotated = len(taxonomic_annotation_60[taxonomic_annotation_60[annotation] != '-'][annotation].notna())\n",
    "    unannotated = total_proteins - annotated\n",
    "    annotation_found.loc[annotation + '_60'] = [annotated, unannotated, total_proteins]\n",
    "\n",
    "# EukProt 90\n",
    "annotated = len(taxonomic_annotation_90_eukprot[taxonomic_annotation_90_eukprot['Name_to_Use'] != '-']['Name_to_Use'].notna())\n",
    "unannotated = total_proteins - annotated\n",
    "annotation_found.loc['eukprot_90'] = [annotated, unannotated, total_proteins]\n",
    "\n",
    "# EukProt 60\n",
    "annotated = len(taxonomic_annotation_60_eukprot[taxonomic_annotation_60_eukprot['Name_to_Use'] != '-']['Name_to_Use'].notna())\n",
    "unannotated = total_proteins - annotated\n",
    "annotation_found.loc['eukprot_60'] = [annotated, unannotated, total_proteins]\n",
    "    \n",
    "# Transform the results into percentage, drop the total_proteins column and rename the index column\n",
    "annotation_found['annotated'] = annotation_found['annotated'] / annotation_found['total_proteins'] * 100\n",
    "annotation_found['unannotated'] = annotation_found['unannotated'] / annotation_found['total_proteins'] * 100\n",
    "annotation_found = annotation_found.drop(columns=['total_proteins'])\n",
    "# Reshape the dataframe into a long format\n",
    "annotation_found = annotation_found.reset_index()\n",
    "annotation_found = annotation_found.melt(id_vars='index', var_name='annotation', value_name='percentage')\n",
    "annotation_found.rename(columns={'index': 'category'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(annotation_found,\n",
    "            x='category', y='percentage', color='annotation',\n",
    "            color_discrete_map={'unannotated': 'gray', 'annotated': 'blue'},\n",
    "            barmode='stack')\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(\n",
    "        family=\"Times New Roman, serif\",  # Set the font family to Times New Roman\n",
    "        size=12,  # Set the font size\n",
    "        color=\"#7f7f7f\"  # Set the font color\n",
    "    ),\n",
    "    autosize=False,\n",
    "    # Convert the width and height from centimeters to pixels\n",
    "    width=int(8 / 2.54 * 96),  # 1 inch = 2.54 cm, 96 pixels per inch\n",
    "    height=int(7 / 2.54 * 96),\n",
    "    margin=dict( # Set the margins\n",
    "        l=0,  # Left margin\n",
    "        r=25,  # Right margin\n",
    "        b=25,  # Bottom margin\n",
    "        t=25  # Top margin\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    yaxis_title='% annotated',\n",
    "    xaxis_tickangle=-90\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.write_image('../../figures/assembly/annotation.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(annotation_found)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the overlap of proteins predicted from the metatranscriptome that have both taxonomic and functional annotations, only one or none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of transcript IDs for each dataframe\n",
    "set1 = set(expression.index)\n",
    "# Read in predicted protein names\n",
    "set2 = pd.read_table('../../data/assembly/protein/SPAdes_mmseqsDB.lookup', header=None)[1]\n",
    "# Cut off the strings that got appended to the transcript names\n",
    "set2 = set([x.split('.')[0] for x in set2])\n",
    "\n",
    "# Create the figure and axes objects\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "# Plot the Venn diagram\n",
    "venn2([set1, set2], ('all', 'predicted proteins'),  ax=ax)\n",
    "\n",
    "# Save the figure as an SVG file\n",
    "plt.savefig('../../figures/assembly/transcript_protein_venn.svg', format='svg', dpi=1200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_annotation_60_eukprot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of transcript IDs for each dataframe\n",
    "set1 = set(expression.index)\n",
    "set2 = set([x.split('.')[0] for x in pd.read_table('../../data/assembly/protein/SPAdes_mmseqsDB.lookup', header=None)[1]])\n",
    "set3 = set(taxonomic_annotation_60['query_id'])\n",
    "set4 = set(functional_annotation['#query'])\n",
    "set5 = set(taxonomic_annotation_60_eukprot['query_id'])\n",
    "set6 = set(taxonomic_annotation_90['query_id'])\n",
    "set7 = set(taxonomic_annotation_90_eukprot['query_id'])\n",
    "\n",
    "labels = venn.get_labels([set2, set3, set4, set5], fill=['number', 'logic'])\n",
    "\n",
    "# Create the figure and axes objects\n",
    "fig, ax = venn.venn2(labels, names=['predicted proteins', 'taxonomic_phyloDB_60', 'functional', 'taxonomic_eukprot_60'])\n",
    "\n",
    "# Save the figure as an SVG file\n",
    "plt.savefig('../../figures/assembly/protein_annotation_venn.svg', format='svg', dpi=1200)\n",
    "\n",
    "# Plot the Venn diagram\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = venn.get_labels([set3, set4, set5], fill=['number', 'logic'])\n",
    "\n",
    "# Create the figure and axes objects\n",
    "fig, ax = venn.venn2(labels, names=['taxonomic_phyloDB_60', 'functional', 'taxonomic_eukprot_60'])\n",
    "\n",
    "# Save the figure as an SVG file\n",
    "plt.savefig('../../figures/assembly/annotation_venn.svg', format='svg', dpi=1200)\n",
    "\n",
    "# Plot the Venn diagram\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same setup, different figure\n",
    "# Create the figure and axes objects\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "# Plot the Venn diagram\n",
    "venn3([set3, set4, set5], ('taxonomic_phyloDB_60', 'functional', 'taxonomic_eukprot_60'),  ax=ax)\n",
    "\n",
    "# Save the figure as an SVG file\n",
    "plt.savefig('../../figures/assembly/annotation_venn_2.svg', format='svg', dpi=1200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = venn.get_labels([set4, set6, set7], fill=['number', 'logic'])\n",
    "\n",
    "# Create the figure and axes objects\n",
    "fig, ax = venn.venn2(labels, names=['functional', 'taxonomic_phyloDB_90', 'taxonomic_eukprot_90'])\n",
    "\n",
    "# Save the figure as an SVG file\n",
    "plt.savefig('../../figures/assembly/annotation_venn_90.svg', format='svg', dpi=1200)\n",
    "\n",
    "# Plot the Venn diagram\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript length distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequnece length file was generated using the get_sequence_lengths.rs script. It is a single-column containing the nucleotide length of every sequence. Let's visualize the distribution of sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sequence length data (a random sample of 10% of the sequences)\n",
    "sequence_length = pd.read_table('../../data/analysis/metatranscriptome_sequence_lengths.txt', header=None)\n",
    "\n",
    "# Read the sequence length data (a random sample of 10% of the sequences)\n",
    "# sequence_length = pd.read_table('../../data/analysis/metatranscriptome_sequence_lengths.txt', header=None,\n",
    "#                                 skiprows=lambda i: i>0 and np.random.rand() > 0.5)\n",
    "sequence_length.columns = ['length']\n",
    "\n",
    "# Plot the distribution of sequence lengths using plotly\n",
    "fig = px.histogram(sequence_length, x='length', log_y=True, nbins=1000, opacity=0.5)\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(\n",
    "        family=\"Times New Roman, serif\",  # Set the font family to Times New Roman\n",
    "        size=8\n",
    "    ),\n",
    "    autosize=False,\n",
    "    # Convert the width and height from centimeters to pixels\n",
    "    width=int(9 / 2.54 * 96),  # 1 inch = 2.54 cm, 96 pixels per inch\n",
    "    height=int(7 / 2.54 * 96),\n",
    "    margin=dict( # Set the margins\n",
    "        l=0,  # Left margin\n",
    "        r=25,  # Right margin\n",
    "        b=25,  # Bottom margin\n",
    "        t=25  # Top margin\n",
    "    ),\n",
    "    yaxis_title='Log number of transcripts',\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.write_image('../../figures/assembly/sequence_length.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform the data\n",
    "sequence_length['length_log'] = np.log10(sequence_length['length'])\n",
    "\n",
    "# Plot the distribution of sequence lengths using plotly\n",
    "fig = px.histogram(sequence_length, x='length_log', log_y=False, nbins=1000, opacity=0.5)\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(\n",
    "        family=\"Times New Roman, serif\",  # Set the font family to Times New Roman\n",
    "        size=8\n",
    "    ),\n",
    "    autosize=False,\n",
    "    # Convert the width and height from centimeters to pixels\n",
    "    width=int(9 / 2.54 * 96),  # 1 inch = 2.54 cm, 96 pixels per inch\n",
    "    height=int(7 / 2.54 * 96),\n",
    "    margin=dict( # Set the margins\n",
    "        l=0,  # Left margin\n",
    "        r=25,  # Right margin\n",
    "        b=25,  # Bottom margin\n",
    "        t=25  # Top margin\n",
    "    ),\n",
    "    yaxis_title='Number of transcripts',\n",
    "    xaxis_title='Log transcript length (bp)'\n",
    ")\n",
    "\n",
    "# Add vertie(x=sequence_length['length_log'].median(), line_width=1, line_dash='dash', line_color='black')\n",
    "# Add value of tcal lines for the median sequence length\n",
    "fig.add_vlinhe most counted sequence length\n",
    "fig.add_annotation(x=sequence_length['length_log'].median(), y=1, text='Median transcript length: 342 bp', \n",
    "                    showarrow=False, textangle=-90, yshift=110, xshift=10, font=dict(size=10))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.write_image('../../figures/assembly/sequence_length_log.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out summary statistics\n",
    "sequence_length = pd.read_table('../../data/analysis/metatranscriptome_sequence_lengths.txt', header=None)\n",
    "sequence_length.columns = ['length']\n",
    "print('Number of transcripts in the metatranscriptome: ' + str(len(sequence_length)))\n",
    "print('Mean transcript length: ' + str(sequence_length['length'].mean()))\n",
    "print('Median transcript length: ' + str(sequence_length['length'].median()))\n",
    "print('Maximum transcript length: ' + str(sequence_length['length'].max()))\n",
    "print('Minimum transcript length: ' + str(sequence_length['length'].min()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression vs Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the squared coefficient of variance from each row\n",
    "coeff_var = (np.std(expression, axis=1) / np.mean(expression, axis=1)) ** 2\n",
    "# Extract the mean expression from each row\n",
    "mean_expression = np.mean(expression, axis=1)\n",
    "# Combine both values in a dataframe\n",
    "df = pd.DataFrame()\n",
    "df['CV2'] = coeff_var\n",
    "df['CV'] = np.sqrt(coeff_var)\n",
    "df['mean_expression'] = mean_expression\n",
    "df['log_mean_expression'] = np.log10(mean_expression + 1)\n",
    "\n",
    "# Subset the dataframe to only include transcripts with a mean expression of at least 1 TPM\n",
    "df = df[df['mean_expression'] >= 1]\n",
    "\n",
    "# Subsample the dataframe to only include 50% of the data for computational efficiency\n",
    "df = df.sample(frac=0.5)\n",
    "\n",
    "fig = px.scatter(df, x='log_mean_expression', \n",
    "                y='CV', opacity=0.2)\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(\n",
    "        family=\"Times New Roman, serif\",  # Set the font family to Times New Roman\n",
    "        size=12,  # Set the font size\n",
    "        color=\"#7f7f7f\"  # Set the font color\n",
    "    ),\n",
    "    autosize=False,\n",
    "    # Convert the width and height from centimeters to pixels\n",
    "    width=int(9 / 2.54 * 96),  # 1 inch = 2.54 cm, 96 pixels per inch\n",
    "    height=int(7 / 2.54 * 96),\n",
    "    margin=dict( # Set the margins\n",
    "        l=0,  # Left margin\n",
    "        r=25,  # Right margin\n",
    "        b=25,  # Bottom margin\n",
    "        t=25  # Top margin\n",
    "    ),\n",
    "    yaxis_title='Coefficient of variance',\n",
    "    xaxis_title='Log average expression of transcripts'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.write_image('../../figures/assembly/variance_vs_log_expression.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
